# XLNet

- **自回归语言模型(Autoregressive LM)**
    - 自回归语言模型就是根据上文内容预测下一个可能跟随的单词, 即自左向右的语言模型任务(或者反过来也行, 就是根据下文预测前面的单词), 这种类型的LM被称为自回归语言模型. GPT就是典型的自回归语言模型. ELMO尽管看上去利用了上文, 也利用了下文, 但是本质上仍然是自回归LM, 这个跟模型具体怎么实现有关系. ELMO是做了两个方向(从左到右以及从右到左两个方向的语言模型), 但是是分别有两个方向的自回归LM, 然后把LSTM的两个方向的隐节点状态拼接到一起, 来体现双向语言模型这个事情的. 所以其实是两个自回归语言模型的拼接, 本质上仍然是自回归语言模型.
    - 缺点: 只能利用上文或者下文的信息, 不能同时利用上文和下文的信息(ELMO这种双向都做, 然后拼接看上去能够解决这个问题, 但因为融合模式过于简单, 所以效果其实并不是太好)
    - 优点: 与下游NLP任务有关, 比如生成类NLP任务, 比如文本摘要, 机器翻译等, 在实际生成内容的时候, 就是从左向右的, 自回归语言模型天然匹配这个过程.(而Bert这种DAE模式, 在生成类NLP任务中, 就面临训练过程和应用过程不一致的问题, 导致生成类的NLP任务到目前为止都做不太好.)

- **自编码语言模型(Autoencoder LM)**
    - 自编码语言模型是一种无监督学习输入的特征的方法, 用一个神经网络把输入(输入通常还会增加一些噪声)变成一个低维的特征, 就是编码部分; 然后再用一个Decoder尝试把特征恢复成原始的信号. 以BERT为例, BERT通过在输入X中随机Mask掉一部分单词, 然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词, 那些被Mask掉的单词就是在输入侧加入的所谓噪音.
    - 优点: 能比较自然地融入双向语言模型, 同时看到被预测单词的上文和下文.
    - 缺点: 由于要在输入侧引入[Mask]标记, 导致预训练阶段和Fine-tuning阶段不一致的问题, 因为Fine-tuning阶段是看不到[Mask]标记的.

- **XLnet**
    - XLNet的思路采用的是自回归语言模型, 根据上文来预测下一个单词, 但是在上文中添加了下文信息, 这样就同时解决了[mask]带来的两阶段不一致问题和无法同时引入上下文信息的问题.