# BERT

BERT属于自编码语言模型(Autoencoder LM)，采用了双向Transformer Encoder结构，并且设计了两个任务来预训练模型.
- 第一个任务是采用MLM的方式来训练语言模型，通俗地说就是在输入一句话的时候，随机地选一些要预测的词，然后用一个特殊的符号[MASK]来代替它们，之后让模型根据所给的标签去学习这些地方该填的词. (**Masked LM**)
- 第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入BERT的两段文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系. (**Next Sentence Prediction**)

BERT相较于RNN、LSTM可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义. 相较于 word2vec，能够根据句子上下文获取词义，从而避免歧义出现. 其缺点在于，模型参数太多，而且模型太大，少量数据训练时，容易过拟合.